#+TITLE: EECS 440 Notes
#+AUTHOR: Stephen Brennan
#+OPTIONS: tex:t
#+STARTUP: entitiespretty

* 2015-08-27 Thursday

** Optimization

*** What is it?

    Find the extreme points of an objective function.

*** Types of Optimization Problems

    - Discrete vs Continuous - objective function is defined on discrete or
      continuous space.
    - Unconstrained vs constrained - whether there are additional constraints
      defining the feasible region.
    - In this class, we are interested in continuous problems, constrained and
      unconstrained.  We use tools from calculus and linear algebra.

*** Unconstrained Optimization

    - Function of one variable, eg minimum of x^2.  Typical method for solving
      this is to compute first and second derivative, find zeros of first
      derivative where second derivative is positive.
    - Fuctions of two variables, you find the same things, but in matrix form:
      - Jacobian \(J = (\frac{\delta{}f}{\delta{}x_i}) = 0\)
      - Hessian \(H = [\frac{\delta^{2}f}{\delta{}x_{i}\delta{}x_j}] > 0\) must be
        positive definite.
    - Can't always do this, due to computational constrains, and due to weird or
      unknown function.

*** Gradient Ascent

    A way of maximizing/minimizing a function.  From your current position
    $\vec{x}$, go in the direction that maximizes the increase.

    \(\vec{x}_{new} = \vec{x}_{old} - \alpha \Delta f_{\vec{x}_old}(\vec{x})\)

    Here, \alpha is the step size, and \Delta f is the function gradient
    evaluated at x_{old}.

    Downside of this is that the convergence rate is not very good.  Also, this
    procedure assumes linearity, where a quadratic function may be a better
    approximation.

*** Newton-Raphson Method

    In this, we use a quadratic approximation of f.  Then, instead of taking a
    linear step, we take a "Newton step".

    \(f(\vec{x}_{old} + u) = f(\vec{x}_{old}) + u^T \Delta f_{\vec{x}_{old}}(\vec{x}) + \frac{1}{2} u^T \Delta^2f_{\vec{x}_{old}}(\vec{x})u = g(u)\)

    More math, see slides.

    Properties:
    - Fast convergence close to solution.
    - Not guaranteed to converge if started far from solution, may cycle or
      diverge in this case.

*** Quasi-Newton Methods

    - Often, constructing the Hessian for a multivariate function is
      computationally difficult, because it takes O(n^2) space and time and has
      to be done over and over.
    - So, a number of methods exist that approximate the Hessian by using the
      Jacobian at nearby points.

*** Local and Global Optima

    - A *global minimum* for a function is a point x where f(x) \leq f(x+u) for
      all u.
    - A *local minimum* is an x where f(x) \leq f(x+u) for all |u|<\epsilon, for
      some positive \epsilon.
    - Every global minimum is a local min, but not the other way around.
    - There is no algorithm that is guaranteed to find the global maximum of an
      arbitrary function.

*** Convex Sets

    Take two points x_1 and x_2.  A point on the line segment between them is
    defined by \lambda x_1 + (1-\lambda) x_2, for 0 \leq \lambda \leq 1.

    A Convex Set is a set of points such that for any two points in the set,
    \lambda x_1 + (1-\lambda) x_2 is also in the set (for 0 \leq \lambda \leq
    1).  Basically, you can visualize these sets on the plane as "shapes that
    don't have holes in them".

*** Convex Functions

    If you look at all the points that are "above" a function - {(x,y)|y \geq
    f(x)}, if that set is convex, then f is a convex function.

    JENSEN'S INEQUALITY (yaaaaaaas)!

    f(\lambda x_1 + (1-\lambda) x_2) \leq \lambda f(x_1) + (1-\lambda) f(x_2)

    Jensen's inequality seems to apply for any convex function.  It just says
    that the points on the segment between f(x_1) and f(x_2) have to be above
    the the function itself.  Pretty cool.

    For a convex function, every local optimum is also a global optimum!  That's
    a pretty nice property to have.

*** Constrained Optimization

    - Minimize a function of x such that some constraints on x are satisfied.
      The constraints define a feasible region on of in which the solution must
      lie.

*** Linear Programming

    Linear Programming is a *special case* of *constrained optimization*, in
    which both the objective function and the constraints are linear!
    Typically, we write all the constraints and objective function as functions
    of matrices and vectors, for compactness.

    When you apply all these linear constraints, you have a feasible region that
    is a "polyhedron" (because it is bounded by a bunch of "hyperplanes").  It's
    possible that one side of the feasible region is open, (so not completely
    bounded).

    If you have a linear objective function, you can say for certain that an
    optimal point is on one of the vertices.

*** Simplex Algorithm

    - Around the polyhedron we go.
    - From any feasible vertex, walk along the edges of the polyhedron,
      following the vertices.
    - Once you are at a vertex where the neighboring vertices have higher f
      values, stop.
    - You've found a local optimum, which happens to be a global optimum since
      the linear function is convex.

    Properties of this algorithm:

    - Very simple, and easy to implement, and works well in practice.
    - It works by traversing vertices, and there may be exponentially many
      vertices for n constraints.  So, in the worst case, runtime is
      exponential.
      - Average case under various distributions has been shown to be
        polynomial, which is useful.
    - Other algorithms exist, such as "interior point methods", which have
      polynomial bounds*

*** Duality in Linear Programming

    From any "primal" LP, we can derive a "dual" LP.  Say we have a primal LP:

    - min_x c^T x, such that
    - A x \geq b
    - x \geq 0

    We could create a dual like this:

    - max_u b^T u, such that
    - A^T u \leq c
    - u \geq 0

    The nice properties of this are:

    - The primal has a solution iff the dual has a solution.
    - Further, the dual LP is a lower bound on the primal LP.
      - That is, if we pick any feasible x and any feasible u, we always havve
        c^T x \geq b^T u.
    - From the relationship between primal and dual LPs, we can derive a set of
      conditions that characterize the solutions for a primal/dual pair, called
      the Karush-Kuhn-Tucker conditions.
    - Essentially, the conditions are that at the optimal solution, x and u are
      feasible and the objective functions c^T x and b^T u are equal (and some
      other stuff).
    - Soumya says if this doesn't make sense now, that's ok.  Which is good,
      because he lost me at the dual being a lower bound on the primal.

*** Summary of Optimization

    - Types of optimization problems.
    - Unconstrained optimization - gradient ascent/descent, Newton Raphson
      methods.
    - Convex sets and functions
    - Constrained optimization:
      - Linear programming
      - Simplex method
      - Duality
      - KKT conditions

** The Simplex Algorithm

   He says we should know how it works.

   Let us consider the following linear program:

   - minimize (with respect to x_1, x_2) f(x) = 3x_1 - 6x_2, such that
   - x_1 + 2x_2 \geq -1
   - 2x_1 + x_2 \geq 0
   - -x_2 + x_1 \geq -1
   - -4x_2 + x_1 \geq -15
   - -4x_1 + x_2 \geq -23
   - x1, x_2 \geq 0

   Steps:
   1. Standardize so everything is in [variables] \geq [constant] form.
   2. Introduce "slack variables".  Essentially, these are the gap in the
      conditions.  These have to be greater than or equal to 0:
      1. x_3 = x_1 + 2x_2 + 1
      2. x_4 = 2x_1 + x_2
      3. x_5 = -x2 + x_1 + 1
      4. x_6 = -4x_2 + x_1 + 15
      5. x_7 = -4x_1 + x_2 + 23
   3. We can put this stuff into tableu form:

      |     | x_1 | x_2 |    |
      | x_3 |   1 |   2 |  1 |
      | x_4 |   2 |   1 |  0 |
      | x_5 |   1 |  -1 |  1 |
      | x_6 |   1 |  -4 | 13 |
      | x_7 |  -4 |   1 | 23 |
      | 2   |   3 |  -6 |  0 |

   4. Assume that zero is feasible.  Pick the variable that will decrease the
      objective function (the most?), and change it accordingly.  In this case,
      we choose x_2.  Then, we write out the constraints, holding x_1 to be 0.
      We find the smallest positive constraint value for x_2, and choose that.
      Whatever variable caused that constraint, we swap it with x_2, and make a
      new tableau.

      In this case, x_5 is the blocking constraint, so we pick it.

      |     | x_1 | x_5 |  1 |
      | x_3 |   3 |  -2 |  3 |
      | x_4 |   3 |  -1 |  1 |
      | x_2 |   1 |  -1 |  1 |
      | x_6 |  -3 |   4 |  9 |
      | x_7 |  -3 |   1 | 24 |
      | z   |  -3 |   6 | -6 |

   5. The value of the function is now -6.  We can see that the right variable
      to decrease now is x_1.  So, we do the constraints again.  Here, the
      blocking constraint is x_6, so then we get this tableau:

      |     |  x_6 | x_5 |   1 |
      | x_3 |   -1 |   2 |  12 |
      | x_4 |   -1 |   3 |  10 |
      | x_2 |  1/3 | 1/3 |   4 |
      | x_1 | -1/3 | 1/3 |   3 |
      | x_7 |    1 |  -5 |  15 |
      | z   |    2 |   1 | -15 |

      The stopping condition is when both variables on top of the columns have
      coefficients that are positive, so you can't improve the function value.

   If you have more than one variable that will decrease the function, you can
   choose any variable to decrease, and you will always get to the correct
   solution.  However, some choices will be faster than others.
* 2015-09-01 Tuesday

  HW1 due tonight at midnight.  HW 2 out today.  Read Ch. 3 in Mitchell.

** What is "Machine Learning?"

   - Machine = autonomous system, with no (or limited) human intervention.
   - Learning?
     - System changes after an experience, so that it can work more effectively
       next time it does the task.
     - We want the system to learn how to do /related/ tasks better too.
   - Specification for a learning system:
     - Given: Task goal, performance measure P, and examples E
     - Produce a *concept* that is good wih respect to P on /all/ examples of
       the task.
   - Example: learn to play chess
     - Perforance measure = games won/lost
     - Examples = games played
     - Concept?  Probably a function mapping a current board state to a move to
       play next.
   - Two phases: learning/training, and evaluation/testing
     - (In the evaluation phase, you want to evaluate on new examples that you
       haven't trained on).
   - Batch learning: one learning phase, with a large set of examples, followed
     by a testing phase.
   - Online learning: examples arrive one at a time (or in small groups);
     learning and evaluation phases iterate.
   - Learning systems need to have some sort of constraint.  Memorizing all the
     examples is probably the best strategy, but we know that this doesn't
     represent learning the underlying concept.

*** Inductive Generalization

    - In all learning problems, need to reason from specific examples to a
      general case.
    - (this is the reverse of deductive reasoning, where you reason from the
      general case to the specific case)
    - Target concept = the underlying concept that the system is trying to
      learn.  EG, Gary kasparov's head.
    - Typically, the performance measure quantifies the difference between
      current and target concepts.
    - Hypothesis space - all concepts the learning system will consider
      (e.g. all possible combinations of animal properties)
    - Hopefully, target concept is in the hypothesis space.
      - But can't include every possible hypothesis in your space.
      - The size would be huge.
      - You would end up memorizing, not learning.
    - This is the idea behind "No Tabula Rasa" (blank slate) learning.  There
      has to be some sort of restriction on hypothesis spaces.
    - Inductive Bias
      - Assumptions used to limit the hypothesis space are the inductive bias.
      - The more assumptions, the stronger the bias.
      - It can even be quantified (later)

*** Learning Settings

**** Supervised Learning

     - Examples are annotated by a teacheer or oracle.
     - Learning system just finds the concept to match the annotations.

**** Unsupervised Learning

     - No annotations
     - Goal is to find interesting patterns in the examples
     - System defines what is interesting.
     - Example: grouping images by content.

**** Semi-Supervised Learning

     - "*normal learning*" is really a combination of the two
     - You do unsupervised learning, and you occasionally get your
       "parent"/oracle to come in and teach you some labels.
     - You use those new concepts to help you organize your thoughts better.

**** Active Learning

     - A few examples are annotated with the target concept.
     - Learning system can "ask" the oracle to label something.
     - There is a cost of labelling that the system must optimize.

**** Transductive Learning

     - Learning system has some knowledge of possible examples it will be
       evaluated on.
     - Adjusts the system to do better on those examples.
     - EG - learn to play chess against Kasparov.

**** Reinforcement Learning

     - This is "sequential" learning.
     - Your environment provides feedback.
     - You take actions and use the consequences to learn.

**** Transfer Learning

     - Human learning is cumulative.
       - When we encounter a new problem, we don't just start from scratch.
       - We use prior knowledge and reasoning.
     - Transfer learning attempts to apply concepts learned in other problems to
       bias your search.

** When to use ML?

   - Shouldn't use ML to recognize geometric shapes.
   - In general, you don't need to learn if you have these things:
     - The concept is already accurately known.
     - It can be easily (and compactly) described
     - Unlikely to change
   - Learning is not free, requires computation and storage, and real world
     effort in labeling, etc.

** Example Representations

   - Internal representation of examples effects how you learn.
   - EG: When you recognize objects, you don't do it at the level of signals on
     your optic nerve.  You do it at the level of smaller parts that you've
     learned.  A chair has four legs, a flat surface, and usually a back.
   - In the same way, pixels aren't useful in object recognition.
   - This is an open area of research: we don't always know the best
     representation of examples.

*** Feature Vector Representation

    - Examples are vectors of values for a set of attributes.
    - Can be an n-by-m matrix

      |      | Attr 1 | Attr 2 | Attr 3 |
      | EG 1 | v_11    | v_12    | v_13    |
      | EG 2 | V_21    | V_22    | v_23    |
      | EG 3 | v_31    | v_32    | v_33    |

    - This is also called "propositional representation", because each example
      can be a logical conjunction.
    - Can represent all the examples as logic formula.

*** Relational Representation
    - Can use first order logic.

*** Multiple Instance Representation
    - Examples are represented by arbitrary sized sets of attribute-value pairs.
* 2015-09-08 Tuesday

** Review:

   - Decision trees: trees where internal nodes are tests on attributes, and
     leaves are class labels.
   - Construct them by choosing attributes which give the most information.
   - Measure this information with entropy, mutual information ("information
     gain").
   - ID3 algorithm is the formal algorithm for applying mutual information to
     constructing decision trees.

** Generalizing ID3

   - What about multiple valued attributes (more than 2-valued)?
     - Mutual information still applies to $v$-valued finite, discrete
       variables.
     - You simply have the internal node for that attribute have $v$ children
       instead of 2.
     - However, the maximum mutual information for a $k$ valued variable is
       $\log{k}$, so the IG function is biased towards attributes with many
       values.
     - Can normalize by dividing by $H(X)$, the entropy of the attribute itself.
       - *Question:* why is this better than dividing by $\log{|X|}$, e.g., the
         maximum overall entropy of $H(X)$?
       - In essence, this division gives you a quantity that answers the
         question "what fraction of this variable's entropy contributes
         information about the class label?"
   - Continuous Attributes
     - Continuous variables have entropy defined on them, but it's useless for
       making a decision in a tree.
     - Need to "bin" the attribute ($X \le v$ or $X \ge v$).
     - You only need to consider values for $v$ that separate different class
       labels in the training set.
       - This is still problematic for large training sets, as we'll see on our
         programming assignment.

   Example

   | Color | Area | Shape    | Class Label |
   | red   |  0.1 | circle   |           1 |
   | red   |  0.7 | square   |           0 |
   | red   |  0.4 | triangle |           1 |
   | blue  |  0.2 | triangle |           1 |
   | blue  |  0.6 | circle   |           0 |
   | blue  |  0.8 | square   |           0 |
   | green |  0.4 | square   |           0 |
   | green |  0.3 | triangle |           0 |
   | green |  0.3 | circle   |           0 |

   1. First, compute H(Y), which is $H(\frac{1}{3})$ (as a shorthand).
   2. Then, compute H(Y|Color):

      \begin{equation}
      H(Y|Color) = p(Color=red)H(Y|Color=red) + p(Color=blue)H(Y|color=blue) + p(Color=green)H(Y|Color=green)
      \end{equation}

      \begin{equation}
      H(Y|Color) = \frac{1}{3}H(\frac{1}{3}) + \frac{1}{3}H(\frac{1}{3}) + \frac{1}{3}\times 0
      \end{equation}

      \begin{equation}
      H(Y|Color) = \frac{2}{3}H(\frac{1}{3})
      \end{equation}

   3. We can use this to compute the information gain of Color.

      \begin{equation}
      IG(Color) = H(Y) - H(Y|Color) = \frac{1}{3} H(\frac{1}{3})
      \end{equation}

   4. Conveniently, this is the same as the information gain of Shape.

   5. For area, if we sort the training set by Area, we find the cutoffs 0.25,
      0.35, and 0.5.  Then we can compute H(Y|Area,v) for each cutoff v.

      $H(Y|Area\le0.25) = \frac{2}{9}\times 0 + \frac{7}{9} H(\frac{1}{7})$, so IG(Area\leq 0.25) = 0.4583

      etc for each cutoff

   6. You choose the best IG, and use that for the root node.  Then continue to
      do this for each child node.

** Overfitting

   - Given enough features, ID3 will create a tree that fits your data perfectly.
     - Enough features = enough that there are no contradictory examples.
   - Overfitting is an issue.

   - What is overfitting?  Making your model too specific to your training
     examples, and not general enough to be applied well to new data.

   - Strictly, if a concept $h$ has:

     - Higher performance on the training examples, but
     - Lower performance on the whole dataset

   - Than some other concept $h'$, then we say that $h$ has overfit the training
     data.

*** Controlling Overfitting

    - Can introduce a restriction on the hypothesis space, to prevent overly
      complex hypotheses from being learned.
    - Early Stopping
      - Standard ID3 algorithm stops when IG(X)=0 for all X.
      - Instead, stop when IG(X) \leq \epsilon, for some chosen \epsilon.
      - This is sensitive to your parameter choice for \epsilon.
      - It's easy to implement, but doesn't work well in practice.
    - Greedy post-pruning
      - Hold aside some training examples at the start.
      - Do your training procedure on the remainder (allowing it to overfit if
        it wants).
      - Then, do a /greedy pruning/ algorithm on your model.
