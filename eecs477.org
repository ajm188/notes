#+TITLE: EECS 477 Notes
#+AUTHOR: Stephen Brennan
#+OPTIONS: tex:t
#+STARTUP: entitiespretty

* 2015-08-24 Monday

  - 5 books, get sections from library
  - 2 tests:
    - Final exam, possibly oral.
    - Midterm
  - 6 homeworks:
    - Need to know octave

** Asymptotics

   Measure time complexity.  Focus is on large inputs.

   - f(n) \in O(g(n)) means "f(n) \leq g(n)"

     \exists c > g, n_0 > 0 s.t. \forall n \geq n_0 : f(n) \leq c g(n)

   - f(n) = \Omega(g(n)) defined: g(n) \in O(f(n))

   - f(n) \in \Theta(g(n)) defined: f(n) \in O(g(n)) and f(n) \in \Omega(g(n))

   - f(n) \in o(g(n)) defined: \(\lim_{n\to\infty} \frac{f(n)}{g(n)} = 0\)

   - f(n) \in \omega(g(n)) defined: \(\lim{n\to\infty} \frac{f(n)}{g(n)}
     =\infty\)

   What is $n$?  Input size.  Sometimes it's a number of elements, or it could
   be multiple parameters (number of nodes, number of edges).

   Sometimes we use the number of bits of the input.  For example, an algorithm
   with input integer $k$.  The number of bits is $n=\Theta(\log k)$.  If the
   runtime is $O(k)$, it looks like it's linear time.  But in the number of
   bits, it's exponential ($O(2^n)$).  It looks polynomial, but it's
   exponential.  It's called pseudo-polynomial.

   Formula:

   $(1-\frac{x}{k})^k$, where \(x \in R\), \(k \in N^+\).  We have that quantity
   \(< e^{-k}\), and \(\geq (1-x)\).  This will be used a lot apparently.

** Optimization Problems

   **Definition:** An instance of an optimization problem is a pair $(X,f)$,
     where $X$ is a set of feasible solutions, and $f$ is an objective function.
     $f$ maps from $X$ to the real numbers.  An /optimal solution/ $x^*$ is an
     element of $X$ with the property that \(f(x^*) \leq f(x) \: \forall x \in
     X\).

   For instance, if you have a graph and you're talking about the minimum
   spanning tree problem, $X$ is the set of all MSTs, and $f$ maps each to the
   sum of the edge weights in the tree.
* TODO 2015-08-26 Wednesday

  Need to copy over notes from paper.

* 2015-08-28 Friday

** Last Time:

   Approximation algorithms have approximation ratio:

   apx ratio = \(max_{I\in{}\mathscr{I}} {\frac{c(I)}{c^*(I)}}\)

   A c-approximation algorithm has cost \leq c \times optimal cost on all instances I of
   the problem $\mathscr{I}$.  One example is the vertex cover problem.  We
   covered a 2-approximation algorithm (called =VCapx=) that operates by
   repeatedly choosing an edge, adding its endpoints to the VC, and removing all
   incident edges from the graph.

   We left off saying that today we would cover the proof that it is a 2-apx
   algorithm.

** Proof

   *Theorem* =VCapx= is a 2-approximation algorithm.

   *Proof* Every edge is covered by =VCapx= at termination.  For every one of
   these edges, the algorithm adds at most two vertices to $V'$.  The optimal
   solution contains at least one of these two.  =VCapx= never considers the
   same vertex twice (since it deletes incident edges).  So, this is a 2
   approximation algorithm.

   Here's the actual text of his proof:

   - Every edge is covered by =VCapx= at terminates.
   - \forall edge chosen by =VCapx=
     - =VCapx= adds 2 vertices to $V'$
     - Opt contains at least one of the two vertices
   - =VCapx= never considers same vertex twice. (by deleting incident edges)
     - \to edges are disjoint, \to $V'$ can be partitioned by edges added by =VCapx=
   - \rightarrow 2-apx algorithm

** Reduction

   The pipeline of reduction:

   (X, c) \in *I* \to (X', c') \in *I' \to* x'^* \to x^*

   If the time to translate (X,c) to (X', c') is T_1, and the time to translate
   x'^* to x^* is T_2, then problem *I* reduces to *I^'* in time T_1 + T_2.

   EG: Any maximization problem reduces to a minimization problem in O(1) time.

***  Optimal Message Passing

     Given a graph G=(V,E) with probability p_e (0 < p_e < 1) associated to each e
     \in E.  Find a spanning tree of G that minimizes the probability of failure.
     (The probabilities are of failure, and independent).

     So, the probability of survival for the whole tree is \Pi_{e\in T} (1-p_e).

     We can reduce the OMP to Minimum Spanning Tree problem in linear time.  We
     define the weight of an edge to be w_e = -\log (1-p_e).  The cost of an MST
     is c(T) = \sum_{e\in T} w_e = \sum_{e\in T} \log 1/(1-p_e) = \log \Pi_{e\in T} 1/(1-p_e) =
     \log 1/(\Pi_{e\in T}(1-p_e)).  Since we're trying to minimize that logarithm, and
     logarithms are strictly increasing functions, we also are minimizing the
     inside of the logarithm.  This is the same as maximizing the denominator,
     which happens to be the probability of survival of the tree.

*** Choosing your reduction

    This isn't necessarily like EECS 343 reductions, where you find the easiest
    reduction to do.  There are entire families of problems that are special
    cases of each other.  A problem might be able to be reduced to the simplest
    of these, or the most general of these.  The reduction to the most general
    problem is usually easiest, and the reduction to the simpler problem is more
    difficult.  The advantage of doing the harder reduction is generally a
    faster algorithm to solve the simpler problem.  It's just a wonderful world
    of tradeoffs here in computer science land.

** GNU Octave

   - Download it via your package manager, or from the GNU website if you're a
     Win/Mac user.
   - There is a good deal of documentation on the GNU site about how to use
     Octave.  It looks like a less powerful Python+NumPy+Matplotlib, or maybe a
     less powerful (open source) Mathematica.
   - =glpk= function for linear programming.
   - First homework this afternoon, due in two weeks!
* 2015-08-31 Monday

** Linear Programming (LP)

   An instance of LP:

   - min \sum_{j=1}^q c_j x_j, subject to:
   - \sum_{j=1}^q a_{ij} x_j \leq b_i, for i = 1, 2, ..., p, and j=1, 2, ..., q
   - x_j > 0

   The constraints define X, the feasible region.  You can switch a minimization
   problem to a maximization problem by negating the objective function.
   Minimization is the "standard form".  You can also define the "slack
   variables" in the constraints, which were covered a bit more in the EECS 440
   lecture on LP.  EG, diet problem:

   | i | Food          | Energy | Protein | Calcium | Price | Max |   |   |   |   |   |   |   |
   | 1 | Oatmeal       |    110 |       4 |       2 |     3 |   4 |   |   |   |   |   |   |   |
   | 2 | Chicken       |    205 |      32 |      12 |    24 |   3 |   |   |   |   |   |   |   |
   | 3 | Eggs          |    160 |      13 |      54 |    13 |   2 |   |   |   |   |   |   |   |
   | 4 | Milk          |    160 |       8 |     285 |     9 |   8 |   |   |   |   |   |   |   |
   | 5 | Pie           |    420 |       4 |      22 |    20 |   2 |   |   |   |   |   |   |   |
   | 6 | Pork w/ beans |    260 |      14 |      80 |    19 |   2 |   |   |   |   |   |   |   |
   |   | GOALS         |   2000 |      55 |     800 |   min |     |   |   |   |   |   |   |   |

   Decision variable is x_1, so here is the problem:

   - minimize, 3x_1 + 24x_2 + 13x_3 + 9x_4 + 20x_5 + 19x_6, subject to:
   - 110x_1 + 205x_2 + 160x_3 + 160x_4 + 420x_5 + 260x_6 \geq 2000
   - 4x_1 + 32x_2 + 13x_3 + 8x_4 + 4x_5 + 14x_6 \geq 55
   - 2x_1 + 12x_2 + 54x_3 + 285x_4 + 22x_5 + 80x_6 \geq 800
   - 0 \leq x_1 \leq 4
   - 0 \leq x_2 \leq 3
   - 0 \leq x_3 \leq 2
   - 0 \leq x_4 \leq 8
   - 0 \leq x_5 \leq 2
   - 0 \leq x_6 \leq 2

   This isn't in standard form due to the greater than or equal to in the top 3
   constraints, and the less than or equal to in the variable bounds.  I guess.

   What to do to get decision variables unrestricted in sign (not in std form):
   If you want x to be negative (or just allowed to be negative) replace it with
   two variables (say, y and z).  Substitute x with y-z, and add the condition
   that y,z \geq 0.  This allows x (aka y-z) to be positive or negative, but you
   could add more conditions on y-z to make it how you'd like.

   The graphical representation of these problems is pretty simple (when you
   have two variables).  The constraints create a nice shaded polygon that
   represents your feasible region, and then you pick the vertex that maximizes
   the objective function.

   **Claim:** There is always an optimal solution in an extreme point.  That's
   worded weird.  I prefer "an optimal solution is always an extreme point."

   You can represent a LP instance in matrix form like this:
   - min C^T x
   - s.t. Ax=b
   - x \geq 0

   Where, x = (x_1, x_2, ..., x_q)^T, c = (c_1, c_2, ..., c_q)^T, A=(a_11, a_12, ..., a_1q;
   ...; a_p1, a_p2, ..., a_pq), b=(b_1, b_2, ..., b_p)^T.

** Integer Linear Programming

   Same as ^, except that the x's must be integers.  Since this is a more
   restricted problem, the solutions are no better than the LP solutions.

** Mixed Integer Linear Programming

   MILP.  Really?

   > Matrix I'd Like to Program - Andrew Mason

   Only some of the decision variables need to be integral, others can be
   continuous.

** Next Time, on Advanced Algorithms:

   Vertex cover, formulated as ILP.
